# Task_05_Descriptive_Stats
Research Task 5 for OPT â€” Using Syracuse Women's Lacrosse 2023 stats to test LLM question answering accuracy with descriptive statistics

## ğŸ“Œ Project Overview
This project is part of **OPT Research Task 5** for the Syracuse University iSchool OPT program.  
The goal is to test the ability of Large Language Models (LLMs) â€” such as ChatGPT, Claude, or GitHub Copilot â€” to correctly answer natural language questions about a small, structured dataset.  

For this task, I used the **Syracuse Women's Lacrosse 2023 Season Stats** dataset.

---

## ğŸ“‚ Dataset
**Source:** [Syracuse Women's Lacrosse Statistics](https://cuse.com/sports/2013/1/16/WLAX_0116134638)  
**Format:** CSV (Cleaned)  
**Columns:**
- Player â€“ Player name
- GP â€“ Games Played
- G â€“ Goals
- A â€“ Assists
- Pts â€“ Points
- Sh â€“ Shots
- Gw â€“ Game-Winning Goals
- GB â€“ Ground Balls
- DC â€“ Draw Controls
- TO â€“ Turnovers
- CT â€“ Caused Turnovers

âš ï¸ **Note:** Original dataset was in PDF format. Cleaned and converted to CSV before use.

---

## ğŸ› ï¸ Methodology
1. **Data Cleaning**
   - Extracted table from PDF.
   - Removed unnecessary rows/columns.
   - Reformatted into clean CSV with one row per player.
   
2. **Validation**
   - Used basic descriptive statistics in Python/Excel to find correct answers for:
     - Top scorer (Goals)
     - Most assists
     - Most points
     - Most possession impact (GB + DC)
     - Most defensive impact (CT)

3. **LLM Prompting**
   - Provided dataset to the LLM.
   - Asked both simple factual and complex judgment questions.
   - Compared LLM responses to validated statistics.

---

## ğŸ’¬ Example Prompts & Correct Answers

Below are examples of the types of questions asked to the LLM, along with the correct answers from the dataset for validation.

---

### **Basic Factual Prompts**
1. **Who scored the most goals?**  
   âœ… *Megan Carney â€“ 59*

2. **Which player had the most assists?**  
   âœ… *Emma Ward â€“ 56*

3. **Who had the highest total points?**  
   âœ… *Meaghan Tyrrell â€“ 107*

4. **Who had the most caused turnovers?**  
   âœ… *Katie Goodale â€“ 27*

---

### **Intermediate Prompts**
5. **Which player contributed most to possession (GB + DC)?**  
   âœ… *Olivia Adamson â€“ 114*

6. **Which player had the highest shooting efficiency (Goals Ã· Shots)?**  
   âœ… *Katelyn Mashewsk â€“ 1.00* (scored on every shot attempted)

---

### **Complex Judgment Prompts**
7. **If the coach wants to win two more games next season, should the focus be on offense or defense? Which one player should be prioritized, and why?**  
   âœ… *Likely focus: Defense* â€“ While the team scores well, improving defensive control and reducing opponent scoring could yield more wins.  
   **Recommended player:** *Katie Goodale* â€“ Leads in caused turnovers (27) and is key for disrupting opponentsâ€™ possession.

---

## ğŸ“Š LLM Test Results

| Question | Correct Answer (Dataset) | LLM Response | Correct? |
|----------|--------------------------|--------------|----------|
| Who scored the most goals? | Megan Carney â€“ 59 | Meaghan Tyrrell | âŒ |
| Which player had the most assists? | Emma Ward â€“ 56 | Emma Ward | âœ… |
| Who had the highest total points? | Meaghan Tyrrell â€“ 107 | Meaghan Tyrrell | âœ… |
| Who had the most caused turnovers? | Katie Goodale â€“ 27 | Emma Tyrrell | âŒ |
| Which player contributed most to possession (GB + DC)? | Olivia Adamson â€“ 114 |  |  |
| Which player had the highest shooting efficiency (Goals Ã· Shots)? | Katelyn Mashewsk â€“ 1.00 |  |  |
| If the coach wants to win two more games next season, should the focus be on offense or defense? Which one player should be prioritized, and why? | **Likely:** Defense â€“ Katie Goodale (to reduce turnovers & improve possession) |  |  |

---

## ğŸ”— LLM Test Conversation Link
[View ChatGPT Conversation](https://chatgpt.com/share/688eb41c-1e5c-8005-b5b6-dc09dcd6e995)


## ğŸ“ˆ Observations

After running the prompts through the LLM and comparing them to the validated dataset answers, the following observations were made:

1. **Basic factual questions** (e.g., top scorer, most assists) were generally answered correctly by the LLM when the dataset was presented in a clear tabular format.
2. **Intermediate questions** (e.g., possession leader, shooting efficiency) required the LLM to perform calculations. Accuracy varied depending on whether the LLM interpreted column definitions correctly.
3. **Complex judgment questions** (e.g., strategy to win more games) were subjective and depended on how the prompt was framed. LLMs tended to make reasonable recommendations but sometimes focused on star players rather than using calculated metrics.
4. The LLM occasionally confused **similar statistical categories** (e.g., total points vs. goals) if column names were not explained clearly in the prompt.
5. **Prompt clarity** greatly improved accuracy. Explicitly stating how to calculate a metric (e.g., â€œadd Ground Balls + Draw Controlsâ€) increased the likelihood of correct results.
6. The LLM performed best when the dataset was **small and fully visible** in the conversation context, allowing it to scan all rows and compute without relying on memory.

**Overall takeaway:**  
LLMs can reliably answer straightforward questions from well-structured datasets, but for complex analysis or multi-step calculations, prompts should be explicit and results should always be validated against actual statistics.
